{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "from utility import Pi_util\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "path_to_lib: str = \"/Users/zacharykelly/Documents/MATLAB/projects/combiExperiments/code/lightLogger/miniSpect\"\n",
    "sys.path.append(path_to_lib)\n",
    "import MS_util\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path: str = \"/Volumes/EXTERNAL1/queue30mins_0.1hz_0NDF\"\n",
    "use_mean_frame: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Pi_util)\n",
    "importlib.reload(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define some helper functions\n",
    "\"\"\"Parser for the raw World data per chunk\"\"\"\n",
    "def world_parser(val_tuple: tuple) -> dict:\n",
    "    print(f'Length of world vals: {len(val_tuple)}')\n",
    "\n",
    "    # First value is always the frame buffer for this chunk \n",
    "    frame_buffer: np.ndarray = val_tuple[0].astype(np.uint8)\n",
    "    \n",
    "    # If we want to only use the mean of each frame, not the entire frame\n",
    "    if(use_mean_frame):\n",
    "        frame_buffer = np.mean(frame_buffer, axis=(2,3))\n",
    "\n",
    "    # Flatten the frame buffer into one chunks worth of frames, instead of per second \n",
    "    frame_buffer = frame_buffer.reshape((frame_buffer.shape[0] * frame_buffer.shape[1], *frame_buffer.shape[2:]))\n",
    "\n",
    "    # Second value is always the settings buffer for this chunk\n",
    "    # The settings are in the format [duration, FPS gain, exposure] TODO: The FPS dimension does not currently exist, but going to add it\n",
    "    settings_buffer: np.ndarray = val_tuple[1].astype(np.float64)\n",
    "\n",
    "    # Third and Fourth values are always the num_captured_frames and observed FPS \n",
    "    num_captured_frames, observed_fps = val_tuple[2:]\n",
    "\n",
    "    print(f'Frame Buffer Shape: {frame_buffer.shape}')\n",
    "    print(f'Captured Frames: {num_captured_frames} | FPS: {observed_fps}')\n",
    "                                                # Make this a float for MATLAB use later\n",
    "    return {'frame_buffer': frame_buffer, 'settings_buffer': settings_buffer, 'num_frames_captured': float(num_captured_frames), 'FPS': observed_fps}\n",
    "\n",
    "\"\"\"Parser for the raw Pupil data per chunk\"\"\"\n",
    "def pupil_parser(val_tuple: tuple) -> dict:\n",
    "    print(f'Length of Pupil vals: {len(val_tuple)}')\n",
    "\n",
    "    # First value is always the frame buffer for this chunk\n",
    "    frame_buffer: np.ndarray = val_tuple[0].astype(np.uint8)\n",
    "    \n",
    "    # If we want to only use the mean of each frame, not the entire frame\n",
    "    if(use_mean_frame):\n",
    "        frame_buffer = np.mean(frame_buffer, axis=(2,3))\n",
    "\n",
    "    # Flatten the frame buffer into one chunks worth of frames, instead of per second \n",
    "    frame_buffer = frame_buffer.reshape((frame_buffer.shape[0] * frame_buffer.shape[1], *frame_buffer.shape[2:]))\n",
    "\n",
    "    # Second value and third value are always num_captured_frames and observed FPS\n",
    "    num_captured_frames, observed_fps = val_tuple[1:]\n",
    "\n",
    "    print(f'Frame Buffer Shape: {frame_buffer.shape}')\n",
    "    print(f'Captured Frames: {num_captured_frames} | FPS: {observed_fps}')\n",
    "                                            # Make this a float for MATLAB use later\n",
    "    return {'frame_buffer': frame_buffer, 'num_frames_captured': float(num_captured_frames), 'FPS': observed_fps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk: 1/180\n",
      "Loading chunk: 2/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/55l_tk897yjbwpmc6_mm1ylm0000gp/T/ipykernel_12060/2860891706.py:28: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  chunk_dict: dict = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(Pi_util)\n",
    "\n",
    "# Define a dictionary of sensor initials and their respective parsers \n",
    "sensor_parsers: dict = {'W': world_parser, \n",
    "                        'P': pupil_parser,\n",
    "                        'M': Pi_util.ms_parser}\n",
    "\n",
    "# First, we must find the gather the sorted paths to the chunks\n",
    "# which are stored in .pkl files\n",
    "chunk_paths: list = natsorted([os.path.join(experiment_path, file) \n",
    "                                for file in os.listdir(experiment_path)\n",
    "                                if '.pkl' in file])\n",
    "\n",
    "# Initialize a list to hold the sorted chunks after \n",
    "# they have been loaded in\n",
    "sorted_chunks: list = []\n",
    "\n",
    "# Next, we will iterate over the chunk files and load them in \n",
    "for chunk_num, path in enumerate(chunk_paths):\n",
    "    if(chunk_num > 1): continue\n",
    "\n",
    "    print(f'Loading chunk: {chunk_num+1}/{len(chunk_paths)}')\n",
    "\n",
    "    # Read in the file and append it to the sorted chunks \n",
    "    # list\n",
    "    with open(path, 'rb') as f:\n",
    "        # Read in the dictionary of values from this chunk\n",
    "        chunk_dict: dict = pickle.load(f)\n",
    "\n",
    "        # Append it to the sorted chunk list \n",
    "        sorted_chunks.append(chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing chunk: 1/2\n",
      "Length of world vals: 4\n",
      "Frame Buffer Shape: (2000,)\n",
      "Captured Frames: 2006 | FPS: 200.50629854303023\n",
      "Length of MS Vals: 3\n",
      "BUFFER TYPE: <class 'numpy.ndarray'>\n",
      "LS CHANNELS: int16 | shape: (10, 60)\n",
      "BUFFER SIZE: 10\n",
      "NUM ACCELERATION COLS: 30\n",
      "NUM ANGULAR COLS: \n",
      "MEASUREMENT SHAPE: (100,)\n",
      "MEASUREMENT SHAPE: (100,)\n",
      "MEASUREMENT SHAPE: (100,)\n",
      "MEASUREMENT SHAPE: (100,)\n",
      "MEASUREMENT SHAPE: (100,)\n",
      "MEASUREMENT SHAPE: (100,)\n",
      "(10, 10)\n",
      "(10, 2)\n",
      "(100, 6)\n",
      "Reading Buffer Shape: (10, 150)\n",
      "Captured Frames: 10 | FPS: 0.9652881621649425\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m parsed_chunk: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Parse this sensor's data with its appropriate sensor\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     parsed_data: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43msensor_parsers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Note this sensor's parsed info for this chunk \u001b[39;00m\n\u001b[1;32m     15\u001b[0m     parsed_chunk[key] \u001b[38;5;241m=\u001b[39m parsed_data\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/combiExperiments/code/lightLogger/raspberry_pi_firmware/utility/Pi_util.py:37\u001b[0m, in \u001b[0;36mms_parser\u001b[0;34m(val_tuple)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading Buffer Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbytes_buffer\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaptured Frames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_captured_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | FPS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobserved_fps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: readings_df \u001b[38;5;28;01mfor\u001b[39;00m name, readings_df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((AS_channels, TS_channels, LS_channels, LS_temp), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m))}\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/combiExperiments/code/lightLogger/raspberry_pi_firmware/utility/Pi_util.py:37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading Buffer Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbytes_buffer\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaptured Frames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_captured_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | FPS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobserved_fps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: readings_df \u001b[38;5;28;01mfor\u001b[39;00m name, readings_df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((AS_channels, TS_channels, LS_channels, LS_temp), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m))}\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'DataFrame'"
     ]
    }
   ],
   "source": [
    "importlib.reload(Pi_util)\n",
    "\n",
    "# Next, we will iterate over the chunks and their sensors and their respective data in the chunk and parse them \n",
    "parsed_chunks: list = []\n",
    "for chunk_num, chunk in enumerate(sorted_chunks):\n",
    "    print(f'Parsing chunk: {chunk_num+1}/{len(sorted_chunks)}')\n",
    "    # initialize a new dictionary to hold sensors' parsed information\n",
    "    parsed_chunk: dict = {}\n",
    "\n",
    "    for key, val in chunk.items():\n",
    "        # Parse this sensor's data with its appropriate sensor\n",
    "        parsed_data: dict = sensor_parsers[key](val)\n",
    "\n",
    "        # Note this sensor's parsed info for this chunk \n",
    "        parsed_chunk[key] = parsed_data\n",
    "\n",
    "    # Append this parsed chunk to the growing list of parsed chunks \n",
    "    parsed_chunks.append(parsed_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Pi_util)\n",
    "parsed = Pi_util.parse_chunks_pkl(experiment_path, use_mean_frame=True)\n",
    "\n",
    "print(parsed[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
